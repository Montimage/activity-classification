{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the training dataset: (279, 22)\n",
      "The shape of the testing dataset: (103, 22)\n",
      "['session_time', '%tcp_protocol', '%udp_protocol', 'ul_data_volume', 'max_ul_volume', 'min_ul_volume', 'avg_ul_volume', 'std_ul_volume', '%ul_volume', 'dl_data_volume', 'max_dl_volume', 'min_dl_volume', 'avg_dl_volume', 'std_dl_volume', '%dl_volume', 'nb_uplink_packet', 'nb_downlink_packet', 'ul_packet', 'dl_packet', 'kB/s', 'nb_packet/s']\n",
      "Preprocessed inputs of the training dataset: [[ 1.24949172  0.59231316 -0.58844871 ...  0.54501086  0.2208743\n",
      "   0.62843195]\n",
      " [-0.8505485   0.59231316 -0.58844871 ...  0.4955518   0.97523057\n",
      "   1.92817113]\n",
      " [ 0.43816388  0.5886868  -0.58480418 ...  0.04242978  0.0580633\n",
      "   0.48885055]\n",
      " ...\n",
      " [-1.2056687   0.34743347 -0.34234218 ... -0.71477922 -0.56070378\n",
      "  -0.41047599]\n",
      " [-0.89460259  0.58409106 -0.58018542 ...  0.10394211  0.44768975\n",
      "   1.24034932]\n",
      " [-0.98176026  0.59231316 -0.58844871 ... -0.62260254  0.43111073\n",
      "  -0.53651579]]\n",
      "Preprocessed outputs of the training dataset: [[1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the training/testing dataset from csv files\n",
    "training_dataset = pd.read_csv(\"../output_train.csv\", header=0, usecols=[*range(1,23)], sep=\";\")\n",
    "testing_dataset = pd.read_csv(\"../output_test.csv\", header=0, usecols=[*range(1,23)], sep=\";\")\n",
    "\n",
    "# Remove missing values\n",
    "training_dataset.dropna(axis = 0, inplace = True)\n",
    "testing_dataset.dropna(axis = 0, inplace = True)\n",
    "\n",
    "print(\"The shape of the training dataset: \" + str(training_dataset.shape))\n",
    "print(\"The shape of the testing dataset: \" + str(testing_dataset.shape))\n",
    "\n",
    "training_dataset.head()\n",
    "\n",
    "# Set of features in the dataset\n",
    "features = list(testing_dataset.columns)\n",
    "print(features[:-1])\n",
    "\n",
    "# Convert the expected output into arrays, e.g., 1 -> [1, 0, 0]\n",
    "output_training = []\n",
    "for i, val in enumerate(training_dataset.iloc[:,21]):\n",
    "    temp = [0, 0, 0]\n",
    "    temp[val - 1] = 1\n",
    "    output_training.append(temp)\n",
    "\n",
    "output_testing = []\n",
    "for i, val in enumerate(testing_dataset.iloc[:,21]):\n",
    "    temp = [0, 0, 0]\n",
    "    temp[val - 1] = 1\n",
    "    output_testing.append(temp)\n",
    "\n",
    "# Remove the expected output column from the datasets\n",
    "prep_training_dataset = training_dataset.drop(columns=['output'])\n",
    "prep_testing_dataset = testing_dataset.drop(columns=['output'])\n",
    "\n",
    "# Preprocessing the data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(prep_training_dataset)\n",
    "\n",
    "# Apply transform to both the training/testing dataset.\n",
    "X_train = scaler.transform(prep_training_dataset)\n",
    "Y_train = np.array(output_training)\n",
    "\n",
    "X_test = scaler.transform(prep_testing_dataset)\n",
    "Y_test = np.array(output_testing)\n",
    "\n",
    "print(\"Preprocessed inputs of the training dataset: \" + str(X_train))\n",
    "print(\"Preprocessed outputs of the training dataset: \" + str(Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23760/1037094140.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mltb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import lightgbm as ltb\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model_ltb = ltb.LGBMRegressor(objective='regression',\n",
    "                              num_leaves=144,\n",
    "                              learning_rate=0.005, \n",
    "                              n_estimators=1000, \n",
    "                              max_depth=12,\n",
    "                              metric='rmse',\n",
    "                              is_training_metric=True,\n",
    "                              max_bin = 55, \n",
    "                              bagging_fraction = 0.8,\n",
    "                              verbose=-1,\n",
    "                              bagging_freq = 5, \n",
    "                              feature_fraction = 0.9)\n",
    "\n",
    "model = ltb.LGBMClassifier()\n",
    "# Using Y_train here got an error of shape\n",
    "model.fit(X_train, training_dataset.iloc[:,21])\n",
    "    \n",
    "Y_expected = testing_dataset.iloc[:,21]\n",
    "Y_predicted = model.predict(X_test)\n",
    "\n",
    "r_2_score = metrics.r2_score(Y_expected, Y_predicted)\n",
    "mean_squared_log_error_score = metrics.mean_squared_log_error(Y_expected, Y_predicted)\n",
    "print(\"r_2 score: %f\" % (r_2_score))\n",
    "print(\"mean_squared_log_error score: %f\" % (mean_squared_log_error_score))\n",
    "\n",
    "wrong_preds = []\n",
    "for i in range(len(Y_expected)):\n",
    "    if Y_predicted[i] != Y_expected[i]:\n",
    "        wrong_preds.append(i)\n",
    "\n",
    "print(\"Wrong predictions: \" + str(wrong_preds))\n",
    "\n",
    "cm = confusion_matrix(Y_expected, Y_predicted)\n",
    "print(\"Confusion matrix: \\n\" + str(cm))\n",
    "\n",
    "labels = [\"Web\", \"Interactive\", \"Video\"]\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "disp.ax_.set_title(\"Confusion Matrix of LightGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import warnings\n",
    "\n",
    "classes=['Web', 'Interactive', 'Video']\n",
    "\n",
    "ltb_explainer = shap.KernelExplainer(model.predict, X_test)\n",
    "#ltb_explainer = shap.TreeExplainer(model)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    shap_values = ltb_explainer.shap_values(X_test)\n",
    "\n",
    "#shap.summary_plot(shap_values[0], X_test, feature_names=features, max_display=24, \n",
    "#                    class_names=classes, color_bar_label='Feature value for Web', plot_type=\"violin\")\n",
    "#shap.summary_plot(shap_values[1], X_test, feature_names=features, max_display=24, \n",
    "#                    class_names=classes, color_bar_label='Feature value for Interactive', plot_type=\"violin\")\n",
    "#shap.summary_plot(shap_values[2], X_test, feature_names=features, max_display=24, \n",
    "#                    class_names=classes, color_bar_label='Feature value for Video', plot_type=\"violin\")\n",
    "\n",
    "shap.summary_plot(shap_values, X_test, feature_names=features, max_display=24, \n",
    "                    class_names=classes, color_bar_label='Feature value for all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency plot between a pair of features ??? \n",
    "# Only for TreeExplainer, not KernelExplainer?\n",
    "ltb_explainer = shap.TreeExplainer(model)\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    shap_values = ltb_explainer.shap_values(X_test)\n",
    "\n",
    "for i in range(len(features[:-1])):\n",
    "    shap.dependence_plot(i, shap_values[0], X_test, feature_names=features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
